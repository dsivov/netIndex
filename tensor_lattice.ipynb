{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensor_lattice.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOq8t/zoGahsY+bYkMsjcYl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dsivov/netIndex/blob/master/tensor_lattice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5PExyksKarX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install -q tensorflow-lattice"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES3v_wOSI42f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy\n",
        "import random\n",
        "import string\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from absl import app\n",
        "from absl import flags\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_lattice as tfl\n",
        "\n",
        "flags_dict = FLAGS._flags()    \n",
        "keys_list = [keys for keys in flags_dict]    \n",
        "for keys in keys_list:\n",
        "      FLAGS.__delattr__(keys)\n",
        "FLAGS = flags.FLAGS\n",
        "flags.DEFINE_integer('num_epochs', 200, 'Number of training epoch.')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmLGgyn8A5Ta",
        "colab_type": "text"
      },
      "source": [
        "Generate 4-char string and put it into numpy. Target is position"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMjJld92IX4L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "7fe46107-7759-4502-c20b-829d12487e46"
      },
      "source": [
        "keys = numpy.empty((0,5), float)\n",
        "\n",
        "keys_list=[]\n",
        "for i in range(1000):\n",
        "    keys_list.append(''.join(random.choices(string.ascii_uppercase +\n",
        "                                 string.digits + string.ascii_lowercase, k=4)))\n",
        "    \n",
        "keys_list.sort()\n",
        "print(keys_list)\n",
        "\n",
        "for i in range(1000):\n",
        "    key = keys_list[i]\n",
        "    key_array = [ord(c)/255 for c in key]\n",
        "    # add position \n",
        "    key_array.append(i/1000)\n",
        "    #print (key_array)\n",
        "    keys = numpy.append(keys, numpy.array([key_array]), axis=0)\n",
        "\n",
        "   "
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['03W4', '03pQ', '04VZ', '07ll', '08Ri', '0CDt', '0EM2', '0NNW', '0NvL', '0TDS', '0VU5', '0VhR', '0WK0', '0dqH', '0mtt', '0nAY', '0oPe', '0quy', '1F0n', '1FQh', '1JUs', '1LMM', '1PA5', '1UEs', '1aeL', '1e2x', '1lhs', '1sK4', '22lI', '22xd', '29Ny', '2Iru', '2JJK', '2KXT', '2cR1', '2g76', '2gu4', '2mdF', '2n6V', '2phb', '2qo5', '2sI5', '2sJC', '2yLD', '2zra', '3HlP', '3Oeg', '3Oin', '3V7M', '3a3g', '3cJK', '3jo0', '3lGi', '440A', '459G', '48oq', '4QMs', '4S56', '4XbH', '4Z0t', '4bef', '4fY8', '4iaZ', '4j9x', '4qwi', '4tGW', '4zGZ', '50sj', '54Jx', '57aG', '5AWO', '5INj', '5Iwl', '5LKf', '5Nxx', '5T5Y', '5UwS', '5XPU', '5jKh', '5sl8', '5uPS', '62lE', '68Hz', '6E8I', '6QjS', '6W6T', '6Zpr', '6cmr', '6gC8', '6jKL', '6k4Y', '6tbQ', '6v1Y', '6yJ5', '75Kl', '75zo', '77iN', '7A0g', '7AtH', '7CUa', '7Ek5', '7HPn', '7Jh1', '7QrC', '7SKU', '7SmO', '7V1p', '7ZSu', '7c3g', '7e26', '7mw2', '7naa', '7qhx', '7vJh', '7vZ4', '7wLZ', '7ybS', '7yq3', '81R1', '8BQc', '8HpV', '8Im2', '8NP1', '8Ogm', '8QmJ', '8S1x', '8T98', '8bCC', '8bat', '8eoS', '8i8P', '8kNX', '8plJ', '8vCT', '91xz', '95UA', '9CRL', '9IeC', '9az9', '9f4D', '9fx0', '9g5o', '9l1M', '9qiz', '9tMB', '9txA', '9xYN', '9yqe', 'A0m9', 'A13X', 'A2dx', 'AGH6', 'AO3n', 'AWqS', 'AYws', 'AZsK', 'AaCR', 'Age5', 'Ahzn', 'Al4G', 'AoGQ', 'Ar9V', 'AuE8', 'AwGj', 'B1EC', 'B8rZ', 'BBTD', 'BHzP', 'BM1S', 'BRMd', 'BSZg', 'BVb9', 'BZbl', 'BbWJ', 'BdDx', 'BlpB', 'BnSJ', 'BnoQ', 'BqRs', 'Btfl', 'BwCW', 'C4H8', 'C4xr', 'CABX', 'CKrw', 'CQBp', 'CTld', 'CVUG', 'CVpW', 'Ca9Q', 'CaZz', 'Cbez', 'Chvc', 'Cnez', 'CnqP', 'CrW4', 'CyMi', 'Cyah', 'Cyuf', 'CzMs', 'D3A4', 'D6ez', 'DC0W', 'DCC1', 'DFkc', 'DL8v', 'DfNj', 'Dg66', 'DlmV', 'DmZF', 'Dmq4', 'DnV9', 'DpyS', 'Drut', 'Dui0', 'DxRc', 'DxvO', 'E0JC', 'E7bO', 'EHn3', 'EHoS', 'EJSU', 'EW5h', 'EWDQ', 'Ep1q', 'EtJe', 'Ev7t', 'ExLa', 'F6ip', 'FC1r', 'FD6q', 'FGGv', 'FJYs', 'FNA4', 'FNn9', 'FNsv', 'FNtz', 'FQHy', 'FVv1', 'FXYx', 'FXn2', 'FY7w', 'FZ2B', 'FbZU', 'FeSE', 'Fgy8', 'Fiju', 'FkhB', 'Flhd', 'Fu3U', 'FvpJ', 'Fvxj', 'FxP0', 'G0t5', 'G4P1', 'GDIP', 'GG0n', 'GP52', 'GSAO', 'GSCT', 'GT85', 'GYDm', 'GcX9', 'Gcb9', 'GgGA', 'Ggoq', 'Gl3G', 'Gl6A', 'GsNo', 'H3T6', 'HBb4', 'HM9d', 'HQns', 'HQpc', 'HUde', 'HXIv', 'HY3b', 'HZ2B', 'HZeg', 'Hctg', 'Hd6Y', 'HeKg', 'Hgto', 'Hiph', 'HjYh', 'Hl8G', 'HpS2', 'Hs54', 'HwZV', 'Hwgp', 'Hxga', 'HzNg', 'I6dF', 'IChX', 'IFdB', 'IGJx', 'IJ6q', 'INsH', 'IQBf', 'IQBh', 'IRhc', 'IUx9', 'IdZ8', 'IeMe', 'ImdE', 'In6c', 'Ioip', 'Isji', 'ItFR', 'ItWK', 'J2fA', 'J6MQ', 'J6lx', 'J6wY', 'JA46', 'JAiQ', 'JBfG', 'JDGW', 'JPYC', 'JTe7', 'JXyW', 'JYFS', 'JgHr', 'Jtke', 'KBqg', 'KBrL', 'KHIb', 'KJhl', 'KKub', 'KO9J', 'KOr9', 'KRBb', 'KSxF', 'KTIB', 'KWW4', 'KYcd', 'KdFC', 'KfFS', 'Ko47', 'KruB', 'Krv6', 'KvJv', 'Kwlb', 'Kx51', 'L26m', 'L7DX', 'LWF1', 'LYjT', 'LkZs', 'LpRY', 'Lttq', 'M2Qt', 'M82M', 'MCKx', 'MEtf', 'MILY', 'MKSm', 'MNhX', 'MQ3P', 'MQPL', 'MTEr', 'MW62', 'MZaD', 'Mea5', 'MgTe', 'Mi0w', 'MiDg', 'MjNA', 'Mv7I', 'Mvaf', 'N03S', 'N3qf', 'NBkj', 'NCGP', 'NGsq', 'NMtA', 'NP2G', 'NQGr', 'Nb8Z', 'Nbcn', 'NcDD', 'Nfvf', 'NiYz', 'NkPm', 'NpE3', 'NqhM', 'NwAg', 'O18o', 'O322', 'O3mq', 'O6IK', 'OFG5', 'OFgV', 'OHRN', 'OLAd', 'ORNN', 'ORjb', 'OW7y', 'ObcI', 'ObqA', 'OeeX', 'OjUg', 'OrAk', 'OtyZ', 'OwAG', 'Oyme', 'P0JF', 'P0Xl', 'PFNB', 'PJbz', 'PP7K', 'PSH9', 'PSuA', 'PTeB', 'PUw2', 'PXdX', 'PXu2', 'PbuU', 'Pddu', 'Peut', 'PfsU', 'Pibx', 'Pkbm', 'PpLh', 'PpPr', 'PtNE', 'PuHy', 'PwIq', 'Pza4', 'Q2nX', 'Q4ZM', 'Q58e', 'QFEy', 'QL9t', 'QLHO', 'QPpX', 'QQ0n', 'QQbg', 'QTQg', 'QWEu', 'QY0a', 'QfI2', 'QgZ6', 'Qji0', 'Qjrr', 'QmA7', 'Qmgz', 'Qrn3', 'QtTW', 'QtzX', 'R07k', 'R3C2', 'R6lG', 'RMFZ', 'RP8U', 'Rb74', 'RriF', 'RwnX', 'S1Jw', 'S3uU', 'SO3S', 'SO8v', 'SRBu', 'SVYk', 'SgYH', 'SnsT', 'Ssel', 'T5jV', 'TGKc', 'TNMg', 'TNeG', 'TRT7', 'TXYy', 'TXba', 'TYsW', 'TcRr', 'TgLL', 'TgMn', 'TgZq', 'Tnpo', 'TpPM', 'TqlI', 'TxAC', 'UD1A', 'UEAZ', 'UIev', 'UJMv', 'UJxe', 'UKoU', 'ULoB', 'UcCe', 'UcDc', 'Uf3I', 'UhHx', 'Uu8J', 'V0Rf', 'V8Au', 'V9Nv', 'VDaF', 'VF0F', 'VHa9', 'VRRv', 'VUpr', 'VcRN', 'VdWS', 'VkRv', 'VpLF', 'VrQw', 'Vu01', 'W3c0', 'W4Gy', 'W5fp', 'W6Xw', 'WIhR', 'WKJ3', 'WKKK', 'WLSI', 'WXzA', 'WZaS', 'Wdlo', 'WeVL', 'WkpR', 'Woig', 'Wowd', 'Wq96', 'Wr28', 'Ws7p', 'Wt3C', 'WuFQ', 'WwfR', 'X6AE', 'XAAK', 'XBFF', 'XFRY', 'XGwa', 'XJoO', 'XNng', 'XYI3', 'Xbvf', 'XhPX', 'XmA7', 'XoXV', 'Xsqv', 'Y6fF', 'YAMb', 'YAXa', 'YEn0', 'YLei', 'YP1P', 'YTKi', 'YVLk', 'YVuS', 'Yfjz', 'Yn6d', 'YoMk', 'Yp7d', 'YwqP', 'YyHY', 'Z7Zb', 'Z8Jf', 'ZV1b', 'ZdSC', 'ZemZ', 'Zi1v', 'Zl3f', 'ZoJP', 'ZvnI', 'a33v', 'a34C', 'a4rg', 'a5V7', 'aGdy', 'aLkh', 'aUJj', 'aVAK', 'aa5j', 'acHu', 'acXB', 'acuG', 'adN0', 'ago3', 'alXB', 'axjv', 'ayVk', 'ayw6', 'b2Ib', 'b4xL', 'b6WT', 'bEZ5', 'bMW3', 'bMh6', 'bQVD', 'bRco', 'bVLx', 'bVnJ', 'biLw', 'bsSA', 'bsjf', 'bwTo', 'c0qS', 'c5IS', 'c5bx', 'cGnj', 'cGyZ', 'cJm8', 'cNJ2', 'cQ6b', 'cc4A', 'cheA', 'ciij', 'ckWB', 'cm9v', 'cqEF', 'cyaP', 'd4iY', 'd8iq', 'dCRB', 'dMuY', 'dRY8', 'dUH5', 'dYmZ', 'ddRW', 'dg2v', 'dp0k', 'dr0Q', 'dvfy', 'e1Zp', 'e6vk', 'e8HO', 'eClL', 'eExy', 'eObl', 'eZoA', 'eZqc', 'eePC', 'efvL', 'ejDx', 'elIR', 'emI9', 'enY2', 'eojp', 'etra', 'fEcC', 'fNoC', 'fROl', 'fWgO', 'fY10', 'fc68', 'fdrE', 'fg4n', 'fsW3', 'fskw', 'ftFZ', 'fv7H', 'g1Kz', 'g3rE', 'g4lo', 'gEPg', 'gF1I', 'gIee', 'gJzG', 'gLNO', 'gUiV', 'gXFX', 'gYkm', 'gcu9', 'ge3F', 'gndC', 'grX6', 'gtOj', 'h08I', 'h0vn', 'h61j', 'h6dV', 'hAE3', 'hEOo', 'hFRe', 'hGDc', 'hJLS', 'hKNI', 'hKTr', 'hQgQ', 'hYAr', 'ha5V', 'hgFL', 'hiQA', 'hjg1', 'hlAo', 'hqWj', 'hzxk', 'i0K0', 'i33L', 'i6Sg', 'i9BH', 'iA8J', 'iDru', 'iE99', 'iFDo', 'iINz', 'iIfo', 'iJaA', 'iOMO', 'iQ61', 'iSrG', 'iaRy', 'iawM', 'idJc', 'ihC7', 'ikk5', 'ioPJ', 'iox9', 'ipAA', 'ipO3', 'iuxq', 'iwYW', 'izar', 'j16S', 'j2QV', 'j3Gi', 'jADU', 'jB9y', 'jD0R', 'jDwm', 'jHYQ', 'jKg4', 'jWoq', 'jXTX', 'jjjO', 'jjsG', 'jobe', 'jwAj', 'jx0j', 'jzWE', 'k3JF', 'k7BA', 'kBI0', 'kEuD', 'kEwY', 'kKlS', 'kM3m', 'kNG5', 'kS7b', 'kTrJ', 'khio', 'khlS', 'kjXW', 'klJ3', 'klWV', 'kpJH', 'kq1T', 'kq1z', 'kxep', 'kz40', 'kzBq', 'l09C', 'l1Zl', 'l3L0', 'l5Xp', 'l73k', 'lBx0', 'lC1q', 'lCJz', 'lEub', 'lH6p', 'lKRR', 'lRun', 'lY58', 'laCZ', 'ldnR', 'ldz6', 'liu3', 'lpph', 'ltcO', 'ltmY', 'lu84', 'lvzv', 'lzsN', 'm0Ml', 'm46V', 'm6Y4', 'mCE0', 'mJYx', 'mMIJ', 'mQAG', 'mZCt', 'mahK', 'meeS', 'miOX', 'mjtX', 'mjuG', 'mkZq', 'mnbB', 'mnqu', 'mqYI', 'mrFs', 'myDj', 'mz1w', 'n8h3', 'n95l', 'nAUu', 'nCSL', 'nEEr', 'nK94', 'nPOM', 'nSrV', 'nZxJ', 'ncwn', 'ndmk', 'nglb', 'niQD', 'nmUD', 'nnIi', 'nqRY', 'nrJk', 'nsY2', 'o292', 'o31r', 'o6iN', 'o6rF', 'o7Eq', 'o7Lo', 'o7lE', 'o9NC', 'oBmF', 'oL5O', 'oQfF', 'oUAO', 'ogSG', 'ok66', 'omwp', 'onW9', 'ooGM', 'orJ7', 'orLa', 'ovEa', 'ox32', 'p7In', 'pA3h', 'pFnp', 'pJeu', 'pQBU', 'pQav', 'pTXj', 'pUCJ', 'pZYV', 'pb6p', 'pe8M', 'pgEn', 'prsf', 'q5ez', 'q85A', 'q9mK', 'qEs9', 'qJh5', 'qMnq', 'qOWf', 'qmSZ', 'qnOl', 'qrYn', 'qumJ', 'qzIL', 'qzJm', 'r1LL', 'r935', 'rB1v', 'rRXe', 'rS6X', 'rTXZ', 'rXpA', 'ra1v', 'rfao', 'rhSS', 'rkQn', 'rked', 'rqGK', 'rs16', 'rzGe', 's0Fo', 's1rj', 's4L1', 's4Yq', 'sAm0', 'sCnD', 'sOfr', 'sOmQ', 'sSbS', 'scOA', 'sf8G', 'shdb', 'sieM', 'snSS', 'soqu', 'spzH', 'sqSJ', 'srR6', 'srYt', 'suuj', 'svec', 'sxJx', 't5IP', 't5S5', 'tCXl', 'tHXD', 'tLdm', 'tOrT', 'tVpH', 'tWb7', 'tXvq', 'tYYT', 'tZEh', 'tbIs', 'teFl', 'teOS', 'tepz', 'tgVh', 'tlkS', 'tmpo', 'tvRz', 'txP5', 'u1Ye', 'u3om', 'u7wv', 'u8vd', 'uCRc', 'uJuL', 'uKWH', 'uMH1', 'uWLP', 'uYTN', 'ubAx', 'udkH', 'ugiA', 'uk2T', 'ulzR', 'umoD', 'urrn', 'uv67', 'uxdh', 'uyV2', 'uzzi', 'vJwk', 'vKVH', 'vLEY', 'vd0h', 'veUv', 'vfbw', 'vgRV', 'vlnx', 'vmUM', 'voND', 'vso0', 'vt0A', 'vxsZ', 'vyCq', 'w6UT', 'wANC', 'wEz1', 'wJKo', 'wXlP', 'wdk8', 'wkcG', 'wolR', 'wsrU', 'wzdI', 'x7WP', 'xAso', 'xBs1', 'xNC9', 'xO1m', 'xbm2', 'xhpF', 'xkCS', 'xn0c', 'xy67', 'xzuJ', 'y1r6', 'y2ek', 'y3aX', 'y9UO', 'yDhW', 'yHF3', 'yHJI', 'yJYq', 'yLnP', 'yVtr', 'yXV7', 'yZCk', 'yaoE', 'ykJt', 'ykhY', 'ytMh', 'yzVO', 'z03v', 'zKG0', 'zSUP', 'zkZA', 'zkhc', 'zwqK', 'zwuO', 'zz0m']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEtxTREwBHzb",
        "colab_type": "text"
      },
      "source": [
        "Create data frame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycm_05fdXc42",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "023d5ded-0e18-4183-8aa8-b92b23d0a52a"
      },
      "source": [
        "df = pd.DataFrame(data=keys, columns=[\"X1\", \"X2\",\"X3\",\"X4\",\"target\"])\n",
        "print(df)\n",
        "combined_calibrators = tfl.layers.ParallelCombination()\n"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           X1        X2        X3        X4  target\n",
            "0    0.188235  0.200000  0.341176  0.203922   0.000\n",
            "1    0.188235  0.200000  0.439216  0.317647   0.001\n",
            "2    0.188235  0.203922  0.337255  0.352941   0.002\n",
            "3    0.188235  0.215686  0.423529  0.423529   0.003\n",
            "4    0.188235  0.219608  0.321569  0.411765   0.004\n",
            "..        ...       ...       ...       ...     ...\n",
            "995  0.478431  0.419608  0.352941  0.254902   0.995\n",
            "996  0.478431  0.419608  0.407843  0.388235   0.996\n",
            "997  0.478431  0.466667  0.443137  0.294118   0.997\n",
            "998  0.478431  0.466667  0.458824  0.309804   0.998\n",
            "999  0.478431  0.478431  0.188235  0.427451   0.999\n",
            "\n",
            "[1000 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBYAl0V_BLyL",
        "colab_type": "text"
      },
      "source": [
        "Define \"monotonicy\" on each X, from 0 to 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Swyi2N5h7ZcZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "calibrator = tfl.layers.PWLCalibration(\n",
        " \n",
        "input_keypoints=np.linspace(df['X1'].min(),\n",
        "                                  df['X1'].max(),\n",
        "                                  num=10),\n",
        "    \n",
        "dtype=tf.float32,     \n",
        "output_min=0.0,\n",
        "output_max=lattice_sizes[0] - 1.0,\n",
        "kernel_initializer='equal_slopes',\n",
        "monotonicity='increasing')\n",
        "combined_calibrators.append(calibrator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VCk-hPhN8NYP",
        "colab": {}
      },
      "source": [
        "calibrator = tfl.layers.PWLCalibration(\n",
        " \n",
        "input_keypoints=np.linspace(df['X2'].min(),\n",
        "                                  df['X2'].max(),\n",
        "                                  num=10),\n",
        "    \n",
        "dtype=tf.float32,     \n",
        "output_min=0.0,\n",
        "output_max=lattice_sizes[0] - 1.0,\n",
        "kernel_initializer='equal_slopes',\n",
        "monotonicity='increasing')\n",
        "combined_calibrators.append(calibrator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5QzWbRYj8PWA",
        "colab": {}
      },
      "source": [
        "calibrator = tfl.layers.PWLCalibration(\n",
        " \n",
        "input_keypoints=np.linspace(df['X3'].min(),\n",
        "                                  df['X3'].max(),\n",
        "                                  num=10),\n",
        "    \n",
        "dtype=tf.float32,     \n",
        "output_min=0.0,\n",
        "output_max=lattice_sizes[0] - 1.0,\n",
        "kernel_initializer='equal_slopes',\n",
        "monotonicity='increasing')\n",
        "combined_calibrators.append(calibrator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UmZXbL6C8Mao",
        "colab": {}
      },
      "source": [
        "calibrator = tfl.layers.PWLCalibration(\n",
        " \n",
        "input_keypoints=np.linspace(df['X4'].min(),\n",
        "                                  df['X4'].max(),\n",
        "                                  num=10),\n",
        "    \n",
        "dtype=tf.float32,     \n",
        "output_min=0.0,\n",
        "output_max=lattice_sizes[0] - 1.0,\n",
        "kernel_initializer='equal_slopes',\n",
        "monotonicity='increasing')\n",
        "combined_calibrators.append(calibrator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUk5mX8uBej6",
        "colab_type": "text"
      },
      "source": [
        "Laticce model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WuKjOXBYFaK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lattice_sizes = [2, 2, 2, 2]\n",
        "lattice = tfl.layers.Lattice(\n",
        "      lattice_sizes=lattice_sizes,\n",
        "      monotonicities=['increasing','increasing', 'increasing',\n",
        "                      'increasing'],\n",
        "      output_min=0.0,\n",
        "      output_max=1.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8myd48cJIOB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "915e277f-9a88-4bf8-ae51-2f14136a6d58"
      },
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(combined_calibrators)\n",
        "model.add(lattice)\n",
        "model.compile(loss=keras.losses.mean_squared_error,\n",
        "                optimizer=keras.optimizers.Adagrad(learning_rate=1.0))\n",
        "\n",
        "features = df[['X1', 'X2', 'X3','X4']].values.astype(np.float32)\n",
        "target = df[['target']].values.astype(np.float32)\n",
        "\n",
        "model.fit(features,\n",
        "  target,\n",
        "  batch_size=32,\n",
        "  epochs=200,\n",
        "  validation_split=0.2,\n",
        "  shuffle=False)\n"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 800 samples, validate on 200 samples\n",
            "Epoch 1/200\n",
            "800/800 [==============================] - 2s 2ms/sample - loss: 0.0104 - val_loss: 0.1380\n",
            "Epoch 2/200\n",
            "800/800 [==============================] - 0s 113us/sample - loss: 0.0210 - val_loss: 0.0290\n",
            "Epoch 3/200\n",
            "800/800 [==============================] - 0s 105us/sample - loss: 0.0137 - val_loss: 0.0280\n",
            "Epoch 4/200\n",
            "800/800 [==============================] - 0s 104us/sample - loss: 0.0135 - val_loss: 0.0318\n",
            "Epoch 5/200\n",
            "800/800 [==============================] - 0s 104us/sample - loss: 0.0121 - val_loss: 0.0259\n",
            "Epoch 6/200\n",
            "800/800 [==============================] - 0s 108us/sample - loss: 0.0121 - val_loss: 0.0322\n",
            "Epoch 7/200\n",
            "800/800 [==============================] - 0s 100us/sample - loss: 0.0149 - val_loss: 0.0484\n",
            "Epoch 8/200\n",
            "800/800 [==============================] - 0s 109us/sample - loss: 0.0116 - val_loss: 0.0508\n",
            "Epoch 9/200\n",
            "800/800 [==============================] - 0s 105us/sample - loss: 0.0118 - val_loss: 0.0332\n",
            "Epoch 10/200\n",
            "800/800 [==============================] - 0s 108us/sample - loss: 0.0103 - val_loss: 0.0409\n",
            "Epoch 11/200\n",
            "800/800 [==============================] - 0s 109us/sample - loss: 0.0076 - val_loss: 0.0436\n",
            "Epoch 12/200\n",
            "800/800 [==============================] - 0s 107us/sample - loss: 0.0076 - val_loss: 0.0201\n",
            "Epoch 13/200\n",
            "800/800 [==============================] - 0s 101us/sample - loss: 0.0100 - val_loss: 0.0414\n",
            "Epoch 14/200\n",
            "800/800 [==============================] - 0s 107us/sample - loss: 0.0127 - val_loss: 0.0198\n",
            "Epoch 15/200\n",
            "800/800 [==============================] - 0s 110us/sample - loss: 0.0116 - val_loss: 0.0207\n",
            "Epoch 16/200\n",
            "800/800 [==============================] - 0s 102us/sample - loss: 0.0109 - val_loss: 0.0164\n",
            "Epoch 17/200\n",
            "800/800 [==============================] - 0s 102us/sample - loss: 0.0115 - val_loss: 0.0144\n",
            "Epoch 18/200\n",
            "800/800 [==============================] - 0s 103us/sample - loss: 0.0101 - val_loss: 0.0140\n",
            "Epoch 19/200\n",
            "800/800 [==============================] - 0s 101us/sample - loss: 0.0081 - val_loss: 0.0168\n",
            "Epoch 20/200\n",
            "800/800 [==============================] - 0s 106us/sample - loss: 0.0063 - val_loss: 0.0270\n",
            "Epoch 21/200\n",
            "800/800 [==============================] - 0s 105us/sample - loss: 0.0045 - val_loss: 0.0543\n",
            "Epoch 22/200\n",
            "800/800 [==============================] - 0s 102us/sample - loss: 0.0118 - val_loss: 0.0167\n",
            "Epoch 23/200\n",
            "800/800 [==============================] - 0s 102us/sample - loss: 0.0077 - val_loss: 0.0161\n",
            "Epoch 24/200\n",
            "800/800 [==============================] - 0s 104us/sample - loss: 0.0042 - val_loss: 0.0317\n",
            "Epoch 25/200\n",
            "800/800 [==============================] - 0s 101us/sample - loss: 0.0074 - val_loss: 0.0223\n",
            "Epoch 26/200\n",
            "800/800 [==============================] - 0s 111us/sample - loss: 0.0043 - val_loss: 0.0251\n",
            "Epoch 27/200\n",
            "800/800 [==============================] - 0s 105us/sample - loss: 0.0022 - val_loss: 0.0256\n",
            "Epoch 28/200\n",
            "800/800 [==============================] - 0s 101us/sample - loss: 0.0052 - val_loss: 0.0227\n",
            "Epoch 29/200\n",
            "800/800 [==============================] - 0s 100us/sample - loss: 0.0022 - val_loss: 0.0271\n",
            "Epoch 30/200\n",
            "800/800 [==============================] - 0s 102us/sample - loss: 0.0051 - val_loss: 0.0225\n",
            "Epoch 31/200\n",
            "800/800 [==============================] - 0s 102us/sample - loss: 0.0019 - val_loss: 0.0223\n",
            "Epoch 32/200\n",
            "800/800 [==============================] - 0s 106us/sample - loss: 0.0081 - val_loss: 0.0023\n",
            "Epoch 33/200\n",
            "800/800 [==============================] - 0s 104us/sample - loss: 0.0099 - val_loss: 0.0027\n",
            "Epoch 34/200\n",
            "800/800 [==============================] - 0s 108us/sample - loss: 0.0113 - val_loss: 0.0061\n",
            "Epoch 35/200\n",
            "800/800 [==============================] - 0s 100us/sample - loss: 0.0125 - val_loss: 0.0067\n",
            "Epoch 36/200\n",
            "800/800 [==============================] - 0s 103us/sample - loss: 0.0094 - val_loss: 0.0909\n",
            "Epoch 37/200\n",
            "800/800 [==============================] - 0s 101us/sample - loss: 0.0177 - val_loss: 0.0105\n",
            "Epoch 38/200\n",
            "800/800 [==============================] - 0s 97us/sample - loss: 0.0133 - val_loss: 0.0060\n",
            "Epoch 39/200\n",
            "800/800 [==============================] - 0s 101us/sample - loss: 0.0097 - val_loss: 0.0211\n",
            "Epoch 40/200\n",
            "800/800 [==============================] - 0s 100us/sample - loss: 0.0031 - val_loss: 0.0156\n",
            "Epoch 41/200\n",
            "800/800 [==============================] - 0s 102us/sample - loss: 0.0020 - val_loss: 0.0189\n",
            "Epoch 42/200\n",
            "800/800 [==============================] - 0s 101us/sample - loss: 0.0021 - val_loss: 0.0216\n",
            "Epoch 43/200\n",
            "800/800 [==============================] - 0s 100us/sample - loss: 0.0021 - val_loss: 0.0225\n",
            "Epoch 44/200\n",
            "800/800 [==============================] - 0s 106us/sample - loss: 0.0022 - val_loss: 0.0227\n",
            "Epoch 45/200\n",
            "800/800 [==============================] - 0s 101us/sample - loss: 0.0022 - val_loss: 0.0228\n",
            "Epoch 46/200\n",
            "800/800 [==============================] - 0s 100us/sample - loss: 0.0021 - val_loss: 0.0231\n",
            "Epoch 47/200\n",
            "800/800 [==============================] - 0s 103us/sample - loss: 0.0019 - val_loss: 0.0233\n",
            "Epoch 48/200\n",
            "800/800 [==============================] - 0s 101us/sample - loss: 0.0013 - val_loss: 0.0222\n",
            "Epoch 49/200\n",
            "800/800 [==============================] - 0s 102us/sample - loss: 0.0024 - val_loss: 0.0445\n",
            "Epoch 50/200\n",
            "800/800 [==============================] - 0s 115us/sample - loss: 0.0027 - val_loss: 0.0307\n",
            "Epoch 51/200\n",
            "800/800 [==============================] - 0s 109us/sample - loss: 0.0011 - val_loss: 0.0440\n",
            "Epoch 52/200\n",
            "800/800 [==============================] - 0s 121us/sample - loss: 0.0031 - val_loss: 0.0263\n",
            "Epoch 53/200\n",
            "800/800 [==============================] - 0s 105us/sample - loss: 4.6234e-04 - val_loss: 0.0302\n",
            "Epoch 54/200\n",
            "800/800 [==============================] - 0s 100us/sample - loss: 0.0022 - val_loss: 0.0221\n",
            "Epoch 55/200\n",
            "800/800 [==============================] - 0s 99us/sample - loss: 0.0011 - val_loss: 0.0188\n",
            "Epoch 56/200\n",
            "800/800 [==============================] - 0s 105us/sample - loss: 0.0017 - val_loss: 0.0190\n",
            "Epoch 57/200\n",
            "800/800 [==============================] - 0s 102us/sample - loss: 0.0016 - val_loss: 0.0188\n",
            "Epoch 58/200\n",
            "800/800 [==============================] - 0s 104us/sample - loss: 0.0016 - val_loss: 0.0184\n",
            "Epoch 59/200\n",
            "800/800 [==============================] - 0s 111us/sample - loss: 0.0015 - val_loss: 0.0181\n",
            "Epoch 60/200\n",
            "800/800 [==============================] - 0s 99us/sample - loss: 0.0014 - val_loss: 0.0178\n",
            "Epoch 61/200\n",
            "800/800 [==============================] - 0s 103us/sample - loss: 0.0014 - val_loss: 0.0176\n",
            "Epoch 62/200\n",
            "800/800 [==============================] - 0s 104us/sample - loss: 0.0013 - val_loss: 0.0174\n",
            "Epoch 63/200\n",
            "800/800 [==============================] - 0s 101us/sample - loss: 0.0013 - val_loss: 0.0173\n",
            "Epoch 64/200\n",
            "800/800 [==============================] - 0s 103us/sample - loss: 0.0012 - val_loss: 0.0173\n",
            "Epoch 65/200\n",
            "800/800 [==============================] - 0s 111us/sample - loss: 0.0012 - val_loss: 0.0173\n",
            "Epoch 66/200\n",
            "800/800 [==============================] - 0s 98us/sample - loss: 0.0012 - val_loss: 0.0181\n",
            "Epoch 67/200\n",
            "800/800 [==============================] - 0s 102us/sample - loss: 0.0012 - val_loss: 0.0185\n",
            "Epoch 68/200\n",
            "800/800 [==============================] - 0s 103us/sample - loss: 0.0012 - val_loss: 0.0174\n",
            "Epoch 69/200\n",
            "800/800 [==============================] - 0s 98us/sample - loss: 0.0011 - val_loss: 0.0185\n",
            "Epoch 70/200\n",
            "800/800 [==============================] - 0s 100us/sample - loss: 0.0011 - val_loss: 0.0188\n",
            "Epoch 71/200\n",
            "800/800 [==============================] - 0s 114us/sample - loss: 0.0011 - val_loss: 0.0175\n",
            "Epoch 72/200\n",
            "800/800 [==============================] - 0s 106us/sample - loss: 0.0011 - val_loss: 0.0177\n",
            "Epoch 73/200\n",
            "800/800 [==============================] - 0s 100us/sample - loss: 0.0011 - val_loss: 0.0188\n",
            "Epoch 74/200\n",
            "800/800 [==============================] - 0s 102us/sample - loss: 0.0011 - val_loss: 0.0177\n",
            "Epoch 75/200\n",
            "800/800 [==============================] - 0s 103us/sample - loss: 0.0011 - val_loss: 0.0178\n",
            "Epoch 76/200\n",
            "800/800 [==============================] - 0s 103us/sample - loss: 0.0011 - val_loss: 0.0178\n",
            "Epoch 77/200\n",
            "800/800 [==============================] - 0s 111us/sample - loss: 0.0010 - val_loss: 0.0178\n",
            "Epoch 78/200\n",
            "800/800 [==============================] - 0s 103us/sample - loss: 0.0010 - val_loss: 0.0188\n",
            "Epoch 79/200\n",
            "800/800 [==============================] - 0s 103us/sample - loss: 0.0019 - val_loss: 0.0066\n",
            "Epoch 80/200\n",
            "800/800 [==============================] - 0s 104us/sample - loss: 0.0036 - val_loss: 0.0024\n",
            "Epoch 81/200\n",
            "800/800 [==============================] - 0s 97us/sample - loss: 0.0066 - val_loss: 0.0027\n",
            "Epoch 82/200\n",
            "800/800 [==============================] - 0s 98us/sample - loss: 0.0058 - val_loss: 0.0063\n",
            "Epoch 83/200\n",
            "800/800 [==============================] - 0s 107us/sample - loss: 0.0062 - val_loss: 0.0072\n",
            "Epoch 84/200\n",
            "800/800 [==============================] - 0s 98us/sample - loss: 0.0042 - val_loss: 0.0070\n",
            "Epoch 85/200\n",
            "800/800 [==============================] - 0s 100us/sample - loss: 0.0026 - val_loss: 0.0068\n",
            "Epoch 86/200\n",
            "800/800 [==============================] - 0s 107us/sample - loss: 0.0013 - val_loss: 0.0066\n",
            "Epoch 87/200\n",
            "800/800 [==============================] - 0s 106us/sample - loss: 0.0043 - val_loss: 0.0071\n",
            "Epoch 88/200\n",
            "800/800 [==============================] - 0s 101us/sample - loss: 0.0058 - val_loss: 0.0072\n",
            "Epoch 89/200\n",
            "800/800 [==============================] - 0s 107us/sample - loss: 0.0026 - val_loss: 0.0068\n",
            "Epoch 90/200\n",
            "800/800 [==============================] - 0s 106us/sample - loss: 6.8446e-04 - val_loss: 0.0016\n",
            "Epoch 91/200\n",
            "800/800 [==============================] - 0s 102us/sample - loss: 0.0084 - val_loss: 0.0074\n",
            "Epoch 92/200\n",
            "800/800 [==============================] - 0s 109us/sample - loss: 0.0073 - val_loss: 0.0076\n",
            "Epoch 93/200\n",
            "800/800 [==============================] - 0s 101us/sample - loss: 0.0026 - val_loss: 0.0068\n",
            "Epoch 94/200\n",
            "800/800 [==============================] - 0s 101us/sample - loss: 8.7133e-04 - val_loss: 0.0269\n",
            "Epoch 95/200\n",
            "800/800 [==============================] - 0s 105us/sample - loss: 0.0075 - val_loss: 0.0075\n",
            "Epoch 96/200\n",
            "800/800 [==============================] - 0s 106us/sample - loss: 0.0056 - val_loss: 0.0073\n",
            "Epoch 97/200\n",
            "800/800 [==============================] - 0s 105us/sample - loss: 0.0017 - val_loss: 0.0303\n",
            "Epoch 98/200\n",
            "800/800 [==============================] - 0s 104us/sample - loss: 0.0065 - val_loss: 0.0076\n",
            "Epoch 99/200\n",
            "800/800 [==============================] - 0s 101us/sample - loss: 0.0049 - val_loss: 0.0073\n",
            "Epoch 100/200\n",
            "800/800 [==============================] - 0s 105us/sample - loss: 0.0018 - val_loss: 0.0330\n",
            "Epoch 101/200\n",
            "800/800 [==============================] - 0s 99us/sample - loss: 0.0060 - val_loss: 0.0076\n",
            "Epoch 102/200\n",
            "800/800 [==============================] - 0s 107us/sample - loss: 0.0050 - val_loss: 0.0074\n",
            "Epoch 103/200\n",
            "800/800 [==============================] - 0s 105us/sample - loss: 0.0019 - val_loss: 0.0345\n",
            "Epoch 104/200\n",
            "800/800 [==============================] - 0s 101us/sample - loss: 0.0054 - val_loss: 0.0077\n",
            "Epoch 105/200\n",
            "800/800 [==============================] - 0s 108us/sample - loss: 0.0048 - val_loss: 0.0074\n",
            "Epoch 106/200\n",
            "800/800 [==============================] - 0s 104us/sample - loss: 0.0018 - val_loss: 0.0346\n",
            "Epoch 107/200\n",
            "800/800 [==============================] - 0s 107us/sample - loss: 0.0046 - val_loss: 0.0077\n",
            "Epoch 108/200\n",
            "800/800 [==============================] - 0s 108us/sample - loss: 0.0046 - val_loss: 0.0075\n",
            "Epoch 109/200\n",
            "800/800 [==============================] - 0s 103us/sample - loss: 0.0017 - val_loss: 0.0342\n",
            "Epoch 110/200\n",
            "800/800 [==============================] - 0s 103us/sample - loss: 0.0041 - val_loss: 0.0077\n",
            "Epoch 111/200\n",
            "800/800 [==============================] - 0s 126us/sample - loss: 0.0044 - val_loss: 0.0075\n",
            "Epoch 112/200\n",
            "800/800 [==============================] - 0s 106us/sample - loss: 0.0016 - val_loss: 0.0337\n",
            "Epoch 113/200\n",
            "800/800 [==============================] - 0s 101us/sample - loss: 0.0037 - val_loss: 0.0077\n",
            "Epoch 114/200\n",
            "800/800 [==============================] - 0s 108us/sample - loss: 0.0041 - val_loss: 0.0075\n",
            "Epoch 115/200\n",
            "800/800 [==============================] - 0s 105us/sample - loss: 0.0014 - val_loss: 0.0320\n",
            "Epoch 116/200\n",
            "800/800 [==============================] - 0s 103us/sample - loss: 0.0034 - val_loss: 0.0077\n",
            "Epoch 117/200\n",
            "800/800 [==============================] - 0s 109us/sample - loss: 0.0037 - val_loss: 0.0074\n",
            "Epoch 118/200\n",
            "800/800 [==============================] - 0s 105us/sample - loss: 0.0011 - val_loss: 0.0286\n",
            "Epoch 119/200\n",
            "800/800 [==============================] - 0s 103us/sample - loss: 0.0033 - val_loss: 0.0077\n",
            "Epoch 120/200\n",
            "800/800 [==============================] - 0s 100us/sample - loss: 0.0033 - val_loss: 0.0074\n",
            "Epoch 121/200\n",
            "800/800 [==============================] - 0s 99us/sample - loss: 8.4139e-04 - val_loss: 0.0244\n",
            "Epoch 122/200\n",
            "800/800 [==============================] - 0s 103us/sample - loss: 0.0032 - val_loss: 0.0076\n",
            "Epoch 123/200\n",
            "800/800 [==============================] - 0s 102us/sample - loss: 0.0028 - val_loss: 0.0073\n",
            "Epoch 124/200\n",
            "800/800 [==============================] - 0s 101us/sample - loss: 5.8448e-04 - val_loss: 0.0204\n",
            "Epoch 125/200\n",
            "800/800 [==============================] - 0s 109us/sample - loss: 0.0031 - val_loss: 0.0076\n",
            "Epoch 126/200\n",
            "800/800 [==============================] - 0s 105us/sample - loss: 0.0024 - val_loss: 0.0072\n",
            "Epoch 127/200\n",
            "800/800 [==============================] - 0s 102us/sample - loss: 8.6085e-04 - val_loss: 0.0070\n",
            "Epoch 128/200\n",
            "800/800 [==============================] - 0s 106us/sample - loss: 5.6603e-04 - val_loss: 0.0068\n",
            "Epoch 129/200\n",
            "800/800 [==============================] - 0s 105us/sample - loss: 2.6686e-04 - val_loss: 0.0067\n",
            "Epoch 130/200\n",
            "800/800 [==============================] - 0s 102us/sample - loss: 1.6024e-04 - val_loss: 0.0062\n",
            "Epoch 131/200\n",
            "800/800 [==============================] - 0s 108us/sample - loss: 3.5569e-04 - val_loss: 0.0068\n",
            "Epoch 132/200\n",
            "800/800 [==============================] - 0s 104us/sample - loss: 2.8168e-04 - val_loss: 0.0067\n",
            "Epoch 133/200\n",
            "800/800 [==============================] - 0s 103us/sample - loss: 2.3674e-04 - val_loss: 0.0067\n",
            "Epoch 134/200\n",
            "800/800 [==============================] - 0s 114us/sample - loss: 2.0134e-04 - val_loss: 0.0067\n",
            "Epoch 135/200\n",
            "800/800 [==============================] - 0s 103us/sample - loss: 1.7421e-04 - val_loss: 0.0066\n",
            "Epoch 136/200\n",
            "800/800 [==============================] - 0s 106us/sample - loss: 1.6384e-04 - val_loss: 0.0066\n",
            "Epoch 137/200\n",
            "800/800 [==============================] - 0s 111us/sample - loss: 1.5880e-04 - val_loss: 0.0066\n",
            "Epoch 138/200\n",
            "800/800 [==============================] - 0s 104us/sample - loss: 1.5537e-04 - val_loss: 0.0066\n",
            "Epoch 139/200\n",
            "800/800 [==============================] - 0s 101us/sample - loss: 1.4376e-04 - val_loss: 0.0066\n",
            "Epoch 140/200\n",
            "800/800 [==============================] - 0s 108us/sample - loss: 1.3514e-04 - val_loss: 0.0066\n",
            "Epoch 141/200\n",
            "800/800 [==============================] - 0s 107us/sample - loss: 1.3177e-04 - val_loss: 0.0066\n",
            "Epoch 142/200\n",
            "800/800 [==============================] - 0s 105us/sample - loss: 1.2928e-04 - val_loss: 0.0066\n",
            "Epoch 143/200\n",
            "800/800 [==============================] - 0s 100us/sample - loss: 1.2786e-04 - val_loss: 0.0066\n",
            "Epoch 144/200\n",
            "800/800 [==============================] - 0s 107us/sample - loss: 1.2943e-04 - val_loss: 0.0066\n",
            "Epoch 145/200\n",
            "800/800 [==============================] - 0s 111us/sample - loss: 1.3507e-04 - val_loss: 0.0066\n",
            "Epoch 146/200\n",
            "800/800 [==============================] - 0s 108us/sample - loss: 1.3581e-04 - val_loss: 0.0066\n",
            "Epoch 147/200\n",
            "800/800 [==============================] - 0s 101us/sample - loss: 1.2579e-04 - val_loss: 0.0066\n",
            "Epoch 148/200\n",
            "800/800 [==============================] - 0s 112us/sample - loss: 1.2516e-04 - val_loss: 0.0066\n",
            "Epoch 149/200\n",
            "800/800 [==============================] - 0s 110us/sample - loss: 1.3745e-04 - val_loss: 0.0066\n",
            "Epoch 150/200\n",
            "800/800 [==============================] - 0s 102us/sample - loss: 1.5871e-04 - val_loss: 0.0066\n",
            "Epoch 151/200\n",
            "800/800 [==============================] - 0s 101us/sample - loss: 1.2079e-04 - val_loss: 0.0064\n",
            "Epoch 152/200\n",
            "800/800 [==============================] - 0s 107us/sample - loss: 9.0011e-05 - val_loss: 0.0014\n",
            "Epoch 153/200\n",
            "800/800 [==============================] - 0s 108us/sample - loss: 5.4841e-04 - val_loss: 0.0273\n",
            "Epoch 154/200\n",
            "800/800 [==============================] - 0s 102us/sample - loss: 0.0015 - val_loss: 0.0074\n",
            "Epoch 155/200\n",
            "800/800 [==============================] - 0s 102us/sample - loss: 0.0012 - val_loss: 0.0336\n",
            "Epoch 156/200\n",
            "800/800 [==============================] - 0s 111us/sample - loss: 8.9146e-04 - val_loss: 0.0076\n",
            "Epoch 157/200\n",
            "800/800 [==============================] - 0s 99us/sample - loss: 0.0020 - val_loss: 0.0482\n",
            "Epoch 158/200\n",
            "800/800 [==============================] - 0s 106us/sample - loss: 0.0029 - val_loss: 0.0077\n",
            "Epoch 159/200\n",
            "800/800 [==============================] - 0s 102us/sample - loss: 0.0016 - val_loss: 0.0407\n",
            "Epoch 160/200\n",
            "800/800 [==============================] - 0s 107us/sample - loss: 5.4787e-04 - val_loss: 0.0079\n",
            "Epoch 161/200\n",
            "800/800 [==============================] - 0s 103us/sample - loss: 0.0025 - val_loss: 0.0580\n",
            "Epoch 162/200\n",
            "800/800 [==============================] - 0s 102us/sample - loss: 0.0014 - val_loss: 0.0082\n",
            "Epoch 163/200\n",
            "800/800 [==============================] - 0s 104us/sample - loss: 0.0024 - val_loss: 0.0588\n",
            "Epoch 164/200\n",
            "800/800 [==============================] - 0s 108us/sample - loss: 0.0013 - val_loss: 0.0082\n",
            "Epoch 165/200\n",
            "800/800 [==============================] - 0s 102us/sample - loss: 0.0023 - val_loss: 0.0578\n",
            "Epoch 166/200\n",
            "800/800 [==============================] - 0s 105us/sample - loss: 0.0011 - val_loss: 0.0082\n",
            "Epoch 167/200\n",
            "800/800 [==============================] - 0s 103us/sample - loss: 0.0023 - val_loss: 0.0582\n",
            "Epoch 168/200\n",
            "800/800 [==============================] - 0s 117us/sample - loss: 9.5282e-04 - val_loss: 0.0081\n",
            "Epoch 169/200\n",
            "800/800 [==============================] - 0s 101us/sample - loss: 0.0025 - val_loss: 0.0611\n",
            "Epoch 170/200\n",
            "800/800 [==============================] - 0s 106us/sample - loss: 7.8559e-04 - val_loss: 0.0082\n",
            "Epoch 171/200\n",
            "800/800 [==============================] - 0s 110us/sample - loss: 0.0026 - val_loss: 0.0649\n",
            "Epoch 172/200\n",
            "800/800 [==============================] - 0s 118us/sample - loss: 6.4225e-04 - val_loss: 0.0082\n",
            "Epoch 173/200\n",
            "800/800 [==============================] - 0s 103us/sample - loss: 0.0029 - val_loss: 0.0696\n",
            "Epoch 174/200\n",
            "800/800 [==============================] - 0s 103us/sample - loss: 5.3033e-04 - val_loss: 0.0082\n",
            "Epoch 175/200\n",
            "800/800 [==============================] - 0s 105us/sample - loss: 0.0027 - val_loss: 0.0785\n",
            "Epoch 176/200\n",
            "800/800 [==============================] - 0s 109us/sample - loss: 4.8381e-04 - val_loss: 0.0084\n",
            "Epoch 177/200\n",
            "800/800 [==============================] - 0s 104us/sample - loss: 0.0027 - val_loss: 0.0900\n",
            "Epoch 178/200\n",
            "800/800 [==============================] - 0s 103us/sample - loss: 4.4281e-04 - val_loss: 0.0085\n",
            "Epoch 179/200\n",
            "800/800 [==============================] - 0s 106us/sample - loss: 0.0028 - val_loss: 0.1051\n",
            "Epoch 180/200\n",
            "800/800 [==============================] - 0s 100us/sample - loss: 4.0171e-04 - val_loss: 0.0085\n",
            "Epoch 181/200\n",
            "800/800 [==============================] - 0s 108us/sample - loss: 0.0030 - val_loss: 0.1242\n",
            "Epoch 182/200\n",
            "800/800 [==============================] - 0s 99us/sample - loss: 3.5984e-04 - val_loss: 0.0085\n",
            "Epoch 183/200\n",
            "800/800 [==============================] - 0s 118us/sample - loss: 0.0033 - val_loss: 0.1483\n",
            "Epoch 184/200\n",
            "800/800 [==============================] - 0s 104us/sample - loss: 3.1492e-04 - val_loss: 0.0056\n",
            "Epoch 185/200\n",
            "800/800 [==============================] - 0s 103us/sample - loss: 0.0033 - val_loss: 0.1779\n",
            "Epoch 186/200\n",
            "800/800 [==============================] - 0s 106us/sample - loss: 2.8548e-04 - val_loss: 0.0031\n",
            "Epoch 187/200\n",
            "800/800 [==============================] - 0s 107us/sample - loss: 0.0032 - val_loss: 0.2124\n",
            "Epoch 188/200\n",
            "800/800 [==============================] - 0s 103us/sample - loss: 2.4881e-04 - val_loss: 0.0035\n",
            "Epoch 189/200\n",
            "800/800 [==============================] - 0s 103us/sample - loss: 0.0031 - val_loss: 0.2168\n",
            "Epoch 190/200\n",
            "800/800 [==============================] - 0s 103us/sample - loss: 2.0419e-04 - val_loss: 0.0064\n",
            "Epoch 191/200\n",
            "800/800 [==============================] - 0s 108us/sample - loss: 0.0025 - val_loss: 0.1888\n",
            "Epoch 192/200\n",
            "800/800 [==============================] - 0s 109us/sample - loss: 1.4801e-04 - val_loss: 0.0119\n",
            "Epoch 193/200\n",
            "800/800 [==============================] - 0s 104us/sample - loss: 0.0019 - val_loss: 0.1653\n",
            "Epoch 194/200\n",
            "800/800 [==============================] - 0s 106us/sample - loss: 1.3958e-04 - val_loss: 0.0192\n",
            "Epoch 195/200\n",
            "800/800 [==============================] - 0s 106us/sample - loss: 0.0016 - val_loss: 0.1437\n",
            "Epoch 196/200\n",
            "800/800 [==============================] - 0s 104us/sample - loss: 1.5967e-04 - val_loss: 0.0267\n",
            "Epoch 197/200\n",
            "800/800 [==============================] - 0s 102us/sample - loss: 0.0014 - val_loss: 0.1242\n",
            "Epoch 198/200\n",
            "800/800 [==============================] - 0s 104us/sample - loss: 1.9980e-04 - val_loss: 0.0328\n",
            "Epoch 199/200\n",
            "800/800 [==============================] - 0s 117us/sample - loss: 0.0012 - val_loss: 0.1065\n",
            "Epoch 200/200\n",
            "800/800 [==============================] - 0s 114us/sample - loss: 2.5450e-04 - val_loss: 0.0385\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe00c084f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFpCWEyiBkK5",
        "colab_type": "text"
      },
      "source": [
        "Predict for each string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1HpOziHKIHo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "95b118db-5ca9-4217-e137-9ddbb957194b"
      },
      "source": [
        "weights = model.layers[0].get_weights()\n",
        "print (weights)"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[0.0000000e+00],\n",
            "       [3.9668623e-02],\n",
            "       [0.0000000e+00],\n",
            "       [1.4431199e-01],\n",
            "       [1.7569473e-01],\n",
            "       [7.0991158e-02],\n",
            "       [8.4668398e-05],\n",
            "       [3.7979960e-02],\n",
            "       [1.3796511e-01],\n",
            "       [0.0000000e+00]], dtype=float32), array([[0.03911368],\n",
            "       [0.0320022 ],\n",
            "       [0.01916919],\n",
            "       [0.0163827 ],\n",
            "       [0.07459845],\n",
            "       [0.12360865],\n",
            "       [0.18055484],\n",
            "       [0.26325846],\n",
            "       [0.15318221],\n",
            "       [0.03764838]], dtype=float32), array([[0.19353238],\n",
            "       [0.12274989],\n",
            "       [0.0552654 ],\n",
            "       [0.04045397],\n",
            "       [0.02678332],\n",
            "       [0.08892113],\n",
            "       [0.08745855],\n",
            "       [0.04790837],\n",
            "       [0.03588909],\n",
            "       [0.02295196]], dtype=float32), array([[1.9711494e-01],\n",
            "       [1.6953975e-01],\n",
            "       [1.3340110e-01],\n",
            "       [4.1132927e-02],\n",
            "       [1.5532970e-03],\n",
            "       [1.3999343e-03],\n",
            "       [3.3503771e-03],\n",
            "       [1.3489723e-03],\n",
            "       [1.0634065e-03],\n",
            "       [8.2015991e-05]], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUuu1Bwq0C2Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "63af13be-28fb-4cbd-ceee-78040a77c6c9"
      },
      "source": [
        "predictions = model.predict(features[:1000])\n",
        "#print(target[:100])\n",
        "#print(features[:100])\n",
        "print(predictions)"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.05409437]\n",
            " [0.05440687]\n",
            " [0.05426188]\n",
            " [0.05450696]\n",
            " [0.05432122]\n",
            " [0.05477293]\n",
            " [0.05465604]\n",
            " [0.0550123 ]\n",
            " [0.05527324]\n",
            " [0.05536845]\n",
            " [0.05555055]\n",
            " [0.05580787]\n",
            " [0.055525  ]\n",
            " [0.0578505 ]\n",
            " [0.05934871]\n",
            " [0.05936579]\n",
            " [0.05953797]\n",
            " [0.05993696]\n",
            " [0.06796996]\n",
            " [0.06814678]\n",
            " [0.06828134]\n",
            " [0.06822404]\n",
            " [0.06811881]\n",
            " [0.06873628]\n",
            " [0.07053341]\n",
            " [0.07102244]\n",
            " [0.07238044]\n",
            " [0.07316661]\n",
            " [0.08088252]\n",
            " [0.08095875]\n",
            " [0.08087267]\n",
            " [0.08169601]\n",
            " [0.08143058]\n",
            " [0.08158638]\n",
            " [0.08384341]\n",
            " [0.08453498]\n",
            " [0.08473225]\n",
            " [0.08570142]\n",
            " [0.08574156]\n",
            " [0.08615381]\n",
            " [0.08624098]\n",
            " [0.08635019]\n",
            " [0.0864048 ]\n",
            " [0.08691466]\n",
            " [0.0870858 ]\n",
            " [0.09490202]\n",
            " [0.09500143]\n",
            " [0.0950127 ]\n",
            " [0.09530073]\n",
            " [0.09677613]\n",
            " [0.09716514]\n",
            " [0.09839804]\n",
            " [0.09867106]\n",
            " [0.10700874]\n",
            " [0.10720297]\n",
            " [0.10760216]\n",
            " [0.10810333]\n",
            " [0.10805985]\n",
            " [0.10896027]\n",
            " [0.10893486]\n",
            " [0.11037951]\n",
            " [0.11090925]\n",
            " [0.11153223]\n",
            " [0.11158039]\n",
            " [0.11269812]\n",
            " [0.11287943]\n",
            " [0.11336853]\n",
            " [0.12067942]\n",
            " [0.12050429]\n",
            " [0.12076083]\n",
            " [0.12114264]\n",
            " [0.12123266]\n",
            " [0.12148435]\n",
            " [0.12126453]\n",
            " [0.12156326]\n",
            " [0.1215819 ]\n",
            " [0.12199309]\n",
            " [0.12206812]\n",
            " [0.12479743]\n",
            " [0.12600917]\n",
            " [0.12615767]\n",
            " [0.13392577]\n",
            " [0.13387088]\n",
            " [0.13434142]\n",
            " [0.13480897]\n",
            " [0.13514712]\n",
            " [0.13573635]\n",
            " [0.13700353]\n",
            " [0.13740402]\n",
            " [0.13799009]\n",
            " [0.13809803]\n",
            " [0.1393215 ]\n",
            " [0.13937642]\n",
            " [0.13956831]\n",
            " [0.14707488]\n",
            " [0.14736989]\n",
            " [0.14733352]\n",
            " [0.14744091]\n",
            " [0.14779499]\n",
            " [0.14769194]\n",
            " [0.14770246]\n",
            " [0.14774153]\n",
            " [0.14774473]\n",
            " [0.14804633]\n",
            " [0.14805165]\n",
            " [0.14825739]\n",
            " [0.14824742]\n",
            " [0.14882627]\n",
            " [0.15001667]\n",
            " [0.15019596]\n",
            " [0.15162149]\n",
            " [0.15183726]\n",
            " [0.15224588]\n",
            " [0.15260164]\n",
            " [0.15255116]\n",
            " [0.1526784 ]\n",
            " [0.15288545]\n",
            " [0.15281269]\n",
            " [0.16001733]\n",
            " [0.16088428]\n",
            " [0.16119409]\n",
            " [0.16100572]\n",
            " [0.16084057]\n",
            " [0.16127925]\n",
            " [0.16131097]\n",
            " [0.16120633]\n",
            " [0.16122985]\n",
            " [0.16308236]\n",
            " [0.16326495]\n",
            " [0.16375422]\n",
            " [0.16424432]\n",
            " [0.16455096]\n",
            " [0.16529584]\n",
            " [0.16577883]\n",
            " [0.1662318 ]\n",
            " [0.1660533 ]\n",
            " [0.16659525]\n",
            " [0.16683197]\n",
            " [0.16876072]\n",
            " [0.16937667]\n",
            " [0.16945814]\n",
            " [0.169589  ]\n",
            " [0.17029372]\n",
            " [0.17110746]\n",
            " [0.17127189]\n",
            " [0.17135738]\n",
            " [0.17163578]\n",
            " [0.17174831]\n",
            " [0.20256728]\n",
            " [0.20242131]\n",
            " [0.20274585]\n",
            " [0.20297903]\n",
            " [0.2032421 ]\n",
            " [0.20409167]\n",
            " [0.20429838]\n",
            " [0.20438497]\n",
            " [0.20511442]\n",
            " [0.20600456]\n",
            " [0.20633319]\n",
            " [0.20668246]\n",
            " [0.20711851]\n",
            " [0.20748918]\n",
            " [0.20762122]\n",
            " [0.20786208]\n",
            " [0.22077626]\n",
            " [0.22124869]\n",
            " [0.2214288 ]\n",
            " [0.22177866]\n",
            " [0.2215029 ]\n",
            " [0.22177982]\n",
            " [0.22197405]\n",
            " [0.22213367]\n",
            " [0.22266346]\n",
            " [0.22359428]\n",
            " [0.22384718]\n",
            " [0.22504245]\n",
            " [0.22524247]\n",
            " [0.22532195]\n",
            " [0.22563015]\n",
            " [0.22593206]\n",
            " [0.2260755 ]\n",
            " [0.23905273]\n",
            " [0.23952562]\n",
            " [0.23972142]\n",
            " [0.24012041]\n",
            " [0.23999447]\n",
            " [0.24044257]\n",
            " [0.24045783]\n",
            " [0.24061261]\n",
            " [0.24167109]\n",
            " [0.2417771 ]\n",
            " [0.24195194]\n",
            " [0.24283135]\n",
            " [0.24355395]\n",
            " [0.24355906]\n",
            " [0.24384996]\n",
            " [0.24444461]\n",
            " [0.24448808]\n",
            " [0.24450427]\n",
            " [0.24451391]\n",
            " [0.257293  ]\n",
            " [0.25783944]\n",
            " [0.25798267]\n",
            " [0.25777638]\n",
            " [0.2583428 ]\n",
            " [0.25823265]\n",
            " [0.2606881 ]\n",
            " [0.2606259 ]\n",
            " [0.26156026]\n",
            " [0.26161593]\n",
            " [0.2615349 ]\n",
            " [0.2616371 ]\n",
            " [0.2620402 ]\n",
            " [0.26226085]\n",
            " [0.2622776 ]\n",
            " [0.26260066]\n",
            " [0.26264513]\n",
            " [0.27577603]\n",
            " [0.27616727]\n",
            " [0.27644414]\n",
            " [0.27669626]\n",
            " [0.27658176]\n",
            " [0.27709854]\n",
            " [0.2771245 ]\n",
            " [0.28015196]\n",
            " [0.2805428 ]\n",
            " [0.28066635]\n",
            " [0.280809  ]\n",
            " [0.29451412]\n",
            " [0.29466727]\n",
            " [0.29473794]\n",
            " [0.29482174]\n",
            " [0.29497424]\n",
            " [0.29462963]\n",
            " [0.29491818]\n",
            " [0.29511398]\n",
            " [0.2951175 ]\n",
            " [0.29496154]\n",
            " [0.2952772 ]\n",
            " [0.2956239 ]\n",
            " [0.2954264 ]\n",
            " [0.29559094]\n",
            " [0.29557228]\n",
            " [0.29673085]\n",
            " [0.29704988]\n",
            " [0.2973259 ]\n",
            " [0.29770502]\n",
            " [0.29787904]\n",
            " [0.29804578]\n",
            " [0.2988013 ]\n",
            " [0.2989365 ]\n",
            " [0.29896352]\n",
            " [0.29884303]\n",
            " [0.3124941 ]\n",
            " [0.3122859 ]\n",
            " [0.3130806 ]\n",
            " [0.31305492]\n",
            " [0.31288713]\n",
            " [0.31342006]\n",
            " [0.3134271 ]\n",
            " [0.3132402 ]\n",
            " [0.31391454]\n",
            " [0.31497723]\n",
            " [0.31502113]\n",
            " [0.31552643]\n",
            " [0.3157067 ]\n",
            " [0.31614327]\n",
            " [0.31612667]\n",
            " [0.31693363]\n",
            " [0.3307261 ]\n",
            " [0.33125836]\n",
            " [0.33154362]\n",
            " [0.33177626]\n",
            " [0.33178064]\n",
            " [0.33205384]\n",
            " [0.33214468]\n",
            " [0.33216816]\n",
            " [0.33217597]\n",
            " [0.33245805]\n",
            " [0.33347493]\n",
            " [0.33344603]\n",
            " [0.33360946]\n",
            " [0.33397228]\n",
            " [0.3342146 ]\n",
            " [0.3342902 ]\n",
            " [0.3344119 ]\n",
            " [0.33469892]\n",
            " [0.3349583 ]\n",
            " [0.33542275]\n",
            " [0.33544612]\n",
            " [0.3355051 ]\n",
            " [0.33558583]\n",
            " [0.34933096]\n",
            " [0.3497746 ]\n",
            " [0.34973305]\n",
            " [0.3496981 ]\n",
            " [0.34970632]\n",
            " [0.3499149 ]\n",
            " [0.34982142]\n",
            " [0.3498215 ]\n",
            " [0.3500467 ]\n",
            " [0.35011137]\n",
            " [0.35153076]\n",
            " [0.35177636]\n",
            " [0.3527202 ]\n",
            " [0.3527918 ]\n",
            " [0.35297513]\n",
            " [0.35332984]\n",
            " [0.35332453]\n",
            " [0.3533404 ]\n",
            " [0.3672336 ]\n",
            " [0.3672565 ]\n",
            " [0.3674302 ]\n",
            " [0.36744863]\n",
            " [0.36726692]\n",
            " [0.36775076]\n",
            " [0.36771917]\n",
            " [0.36764267]\n",
            " [0.36781463]\n",
            " [0.36794472]\n",
            " [0.3684972 ]\n",
            " [0.3683963 ]\n",
            " [0.36996448]\n",
            " [0.37129706]\n",
            " [0.38580507]\n",
            " [0.38577554]\n",
            " [0.38572958]\n",
            " [0.38589704]\n",
            " [0.3859374 ]\n",
            " [0.38575906]\n",
            " [0.3857546 ]\n",
            " [0.38590539]\n",
            " [0.3860898 ]\n",
            " [0.38594675]\n",
            " [0.38602614]\n",
            " [0.386519  ]\n",
            " [0.38747984]\n",
            " [0.38778043]\n",
            " [0.38855752]\n",
            " [0.38904476]\n",
            " [0.3889385 ]\n",
            " [0.3892687 ]\n",
            " [0.38937026]\n",
            " [0.38914576]\n",
            " [0.4032035 ]\n",
            " [0.40330532]\n",
            " [0.4038763 ]\n",
            " [0.40450191]\n",
            " [0.4063364 ]\n",
            " [0.4067767 ]\n",
            " [0.40712172]\n",
            " [0.42128414]\n",
            " [0.42126334]\n",
            " [0.4216848 ]\n",
            " [0.42186642]\n",
            " [0.4217558 ]\n",
            " [0.42182076]\n",
            " [0.42193007]\n",
            " [0.42177895]\n",
            " [0.42183405]\n",
            " [0.42204863]\n",
            " [0.42185348]\n",
            " [0.42249712]\n",
            " [0.4234137 ]\n",
            " [0.42383128]\n",
            " [0.4239875 ]\n",
            " [0.42402685]\n",
            " [0.4240536 ]\n",
            " [0.4250483 ]\n",
            " [0.42512053]\n",
            " [0.4391762 ]\n",
            " [0.439458  ]\n",
            " [0.43981215]\n",
            " [0.43966773]\n",
            " [0.43989515]\n",
            " [0.43984658]\n",
            " [0.43971717]\n",
            " [0.43985564]\n",
            " [0.44119343]\n",
            " [0.44129422]\n",
            " [0.44124162]\n",
            " [0.44173792]\n",
            " [0.44201022]\n",
            " [0.44216797]\n",
            " [0.4423672 ]\n",
            " [0.4427304 ]\n",
            " [0.44304034]\n",
            " [0.45728165]\n",
            " [0.45681682]\n",
            " [0.45746797]\n",
            " [0.4573283 ]\n",
            " [0.4573926 ]\n",
            " [0.4578479 ]\n",
            " [0.45776743]\n",
            " [0.45780757]\n",
            " [0.45790905]\n",
            " [0.45804673]\n",
            " [0.45822355]\n",
            " [0.45921496]\n",
            " [0.45917237]\n",
            " [0.45955038]\n",
            " [0.46002936]\n",
            " [0.4606974 ]\n",
            " [0.4608431 ]\n",
            " [0.46090588]\n",
            " [0.4610834 ]\n",
            " [0.47521877]\n",
            " [0.4753749 ]\n",
            " [0.47564548]\n",
            " [0.47590736]\n",
            " [0.47579604]\n",
            " [0.47572753]\n",
            " [0.47600695]\n",
            " [0.4760467 ]\n",
            " [0.47586906]\n",
            " [0.47637665]\n",
            " [0.47604823]\n",
            " [0.47722328]\n",
            " [0.47741315]\n",
            " [0.47753087]\n",
            " [0.47761297]\n",
            " [0.47790483]\n",
            " [0.47807923]\n",
            " [0.47845316]\n",
            " [0.47845736]\n",
            " [0.4786573 ]\n",
            " [0.4787559 ]\n",
            " [0.47885233]\n",
            " [0.47881347]\n",
            " [0.49326062]\n",
            " [0.49321875]\n",
            " [0.49318025]\n",
            " [0.49354896]\n",
            " [0.49360687]\n",
            " [0.49358457]\n",
            " [0.49375564]\n",
            " [0.49360073]\n",
            " [0.4937644 ]\n",
            " [0.4938568 ]\n",
            " [0.4940127 ]\n",
            " [0.49407405]\n",
            " [0.49494803]\n",
            " [0.49517   ]\n",
            " [0.4953923 ]\n",
            " [0.4957308 ]\n",
            " [0.4956926 ]\n",
            " [0.49596146]\n",
            " [0.49610752]\n",
            " [0.49640897]\n",
            " [0.4964463 ]\n",
            " [0.50920004]\n",
            " [0.50879264]\n",
            " [0.5093549 ]\n",
            " [0.50969267]\n",
            " [0.5097088 ]\n",
            " [0.51059985]\n",
            " [0.51230395]\n",
            " [0.51256704]\n",
            " [0.52533585]\n",
            " [0.52547944]\n",
            " [0.5257542 ]\n",
            " [0.52580535]\n",
            " [0.52588797]\n",
            " [0.52616686]\n",
            " [0.5274322 ]\n",
            " [0.52805847]\n",
            " [0.5283972 ]\n",
            " [0.5415815 ]\n",
            " [0.5418327 ]\n",
            " [0.54189897]\n",
            " [0.54191124]\n",
            " [0.5416788 ]\n",
            " [0.54234505]\n",
            " [0.54236984]\n",
            " [0.54242706]\n",
            " [0.54313534]\n",
            " [0.5434457 ]\n",
            " [0.5434794 ]\n",
            " [0.54351217]\n",
            " [0.5440792 ]\n",
            " [0.5441611 ]\n",
            " [0.54425824]\n",
            " [0.544502  ]\n",
            " [0.5576892 ]\n",
            " [0.55787855]\n",
            " [0.558028  ]\n",
            " [0.5579543 ]\n",
            " [0.5580641 ]\n",
            " [0.5580374 ]\n",
            " [0.55793643]\n",
            " [0.5591633 ]\n",
            " [0.5591632 ]\n",
            " [0.5593544 ]\n",
            " [0.55959177]\n",
            " [0.56041145]\n",
            " [0.5736502 ]\n",
            " [0.5737292 ]\n",
            " [0.573763  ]\n",
            " [0.5739682 ]\n",
            " [0.57385296]\n",
            " [0.5738021 ]\n",
            " [0.5741608 ]\n",
            " [0.57438016]\n",
            " [0.57519394]\n",
            " [0.57530415]\n",
            " [0.57585967]\n",
            " [0.57613796]\n",
            " [0.57633317]\n",
            " [0.57611096]\n",
            " [0.58930564]\n",
            " [0.5897794 ]\n",
            " [0.58988154]\n",
            " [0.5898568 ]\n",
            " [0.5901775 ]\n",
            " [0.5896725 ]\n",
            " [0.5900829 ]\n",
            " [0.5901016 ]\n",
            " [0.5904722 ]\n",
            " [0.5906684 ]\n",
            " [0.59138966]\n",
            " [0.59140027]\n",
            " [0.5919011 ]\n",
            " [0.59217185]\n",
            " [0.5921802 ]\n",
            " [0.59201646]\n",
            " [0.5921003 ]\n",
            " [0.59236395]\n",
            " [0.5923138 ]\n",
            " [0.5924294 ]\n",
            " [0.5925311 ]\n",
            " [0.60579294]\n",
            " [0.6060735 ]\n",
            " [0.60604036]\n",
            " [0.6061741 ]\n",
            " [0.6062825 ]\n",
            " [0.6062661 ]\n",
            " [0.6063223 ]\n",
            " [0.6061925 ]\n",
            " [0.607288  ]\n",
            " [0.607671  ]\n",
            " [0.60776293]\n",
            " [0.60815203]\n",
            " [0.6084008 ]\n",
            " [0.6219866 ]\n",
            " [0.62222344]\n",
            " [0.6222594 ]\n",
            " [0.6217934 ]\n",
            " [0.6223818 ]\n",
            " [0.62227345]\n",
            " [0.6224828 ]\n",
            " [0.62257195]\n",
            " [0.6226297 ]\n",
            " [0.6236075 ]\n",
            " [0.62408894]\n",
            " [0.6241598 ]\n",
            " [0.6242144 ]\n",
            " [0.62451935]\n",
            " [0.62456775]\n",
            " [0.6303252 ]\n",
            " [0.63028646]\n",
            " [0.63077974]\n",
            " [0.63154936]\n",
            " [0.6317489 ]\n",
            " [0.63198185]\n",
            " [0.6321751 ]\n",
            " [0.632351  ]\n",
            " [0.6326666 ]\n",
            " [0.68115354]\n",
            " [0.68100905]\n",
            " [0.6812588 ]\n",
            " [0.68081534]\n",
            " [0.68148834]\n",
            " [0.6815245 ]\n",
            " [0.68164694]\n",
            " [0.6816296 ]\n",
            " [0.68220615]\n",
            " [0.6823378 ]\n",
            " [0.6822352 ]\n",
            " [0.6823127 ]\n",
            " [0.6819231 ]\n",
            " [0.68223524]\n",
            " [0.6827787 ]\n",
            " [0.6833745 ]\n",
            " [0.68339396]\n",
            " [0.68314815]\n",
            " [0.6858177 ]\n",
            " [0.68586373]\n",
            " [0.68586385]\n",
            " [0.685652  ]\n",
            " [0.68564296]\n",
            " [0.68575794]\n",
            " [0.6860429 ]\n",
            " [0.6862304 ]\n",
            " [0.6863295 ]\n",
            " [0.6863246 ]\n",
            " [0.6873411 ]\n",
            " [0.6877279 ]\n",
            " [0.6878507 ]\n",
            " [0.6879531 ]\n",
            " [0.6891804 ]\n",
            " [0.6891583 ]\n",
            " [0.6892384 ]\n",
            " [0.6894735 ]\n",
            " [0.68946695]\n",
            " [0.68913114]\n",
            " [0.68892014]\n",
            " [0.68946505]\n",
            " [0.69014585]\n",
            " [0.6905025 ]\n",
            " [0.69068915]\n",
            " [0.6906687 ]\n",
            " [0.690867  ]\n",
            " [0.6910027 ]\n",
            " [0.6913153 ]\n",
            " [0.6925477 ]\n",
            " [0.69260585]\n",
            " [0.6925746 ]\n",
            " [0.69282806]\n",
            " [0.6925174 ]\n",
            " [0.692513  ]\n",
            " [0.6931288 ]\n",
            " [0.6936709 ]\n",
            " [0.69384646]\n",
            " [0.6943252 ]\n",
            " [0.6944059 ]\n",
            " [0.694564  ]\n",
            " [0.6958567 ]\n",
            " [0.6959349 ]\n",
            " [0.69584596]\n",
            " [0.6960576 ]\n",
            " [0.69613415]\n",
            " [0.69617605]\n",
            " [0.69637215]\n",
            " [0.6965251 ]\n",
            " [0.6969548 ]\n",
            " [0.69712275]\n",
            " [0.6973555 ]\n",
            " [0.6974379 ]\n",
            " [0.69725764]\n",
            " [0.69719017]\n",
            " [0.69762725]\n",
            " [0.6978252 ]\n",
            " [0.6993079 ]\n",
            " [0.6993731 ]\n",
            " [0.6995164 ]\n",
            " [0.69969904]\n",
            " [0.6991646 ]\n",
            " [0.699953  ]\n",
            " [0.7002732 ]\n",
            " [0.70049417]\n",
            " [0.70076406]\n",
            " [0.7011117 ]\n",
            " [0.7011081 ]\n",
            " [0.7011245 ]\n",
            " [0.7024932 ]\n",
            " [0.70245254]\n",
            " [0.70257676]\n",
            " [0.7027409 ]\n",
            " [0.7026467 ]\n",
            " [0.70281076]\n",
            " [0.7027458 ]\n",
            " [0.7027489 ]\n",
            " [0.7029732 ]\n",
            " [0.7030299 ]\n",
            " [0.70313144]\n",
            " [0.7033508 ]\n",
            " [0.7036137 ]\n",
            " [0.7041122 ]\n",
            " [0.7041062 ]\n",
            " [0.70443785]\n",
            " [0.70574266]\n",
            " [0.70588636]\n",
            " [0.705844  ]\n",
            " [0.7059003 ]\n",
            " [0.7055105 ]\n",
            " [0.7060737 ]\n",
            " [0.70608574]\n",
            " [0.7060806 ]\n",
            " [0.70608026]\n",
            " [0.7060442 ]\n",
            " [0.7061233 ]\n",
            " [0.70616513]\n",
            " [0.7064146 ]\n",
            " [0.70677745]\n",
            " [0.7071073 ]\n",
            " [0.70713925]\n",
            " [0.7069158 ]\n",
            " [0.7074116 ]\n",
            " [0.70766234]\n",
            " [0.70793194]\n",
            " [0.70851207]\n",
            " [0.7091061 ]\n",
            " [0.7092228 ]\n",
            " [0.70915854]\n",
            " [0.7093055 ]\n",
            " [0.70945084]\n",
            " [0.7090436 ]\n",
            " [0.70940757]\n",
            " [0.7094344 ]\n",
            " [0.7094767 ]\n",
            " [0.70931095]\n",
            " [0.7094295 ]\n",
            " [0.7088872 ]\n",
            " [0.7095077 ]\n",
            " [0.71013975]\n",
            " [0.7101371 ]\n",
            " [0.71029925]\n",
            " [0.7102144 ]\n",
            " [0.7103719 ]\n",
            " [0.7108338 ]\n",
            " [0.7106589 ]\n",
            " [0.7108048 ]\n",
            " [0.71056765]\n",
            " [0.7111136 ]\n",
            " [0.71114236]\n",
            " [0.7112402 ]\n",
            " [0.710981  ]\n",
            " [0.71101147]\n",
            " [0.7110339 ]\n",
            " [0.71120054]\n",
            " [0.71122897]\n",
            " [0.7111931 ]\n",
            " [0.7113074 ]\n",
            " [0.7112802 ]\n",
            " [0.7108537 ]\n",
            " [0.7115756 ]\n",
            " [0.7115558 ]\n",
            " [0.7124797 ]\n",
            " [0.71244186]\n",
            " [0.7127352 ]\n",
            " [0.71298784]\n",
            " [0.7130067 ]\n",
            " [0.7130029 ]\n",
            " [0.7109256 ]\n",
            " [0.7108907 ]\n",
            " [0.7106142 ]\n",
            " [0.7111832 ]\n",
            " [0.71129346]\n",
            " [0.7113187 ]\n",
            " [0.7112831 ]\n",
            " [0.7108437 ]\n",
            " [0.7113861 ]\n",
            " [0.71141946]\n",
            " [0.7123985 ]\n",
            " [0.7123816 ]\n",
            " [0.7124778 ]\n",
            " [0.7121966 ]\n",
            " [0.7125702 ]\n",
            " [0.7127109 ]\n",
            " [0.7127862 ]\n",
            " [0.7128022 ]\n",
            " [0.71302885]\n",
            " [0.71268594]\n",
            " [0.7130685 ]\n",
            " [0.7108525 ]\n",
            " [0.7110524 ]\n",
            " [0.7103957 ]\n",
            " [0.71108234]\n",
            " [0.71104944]\n",
            " [0.71069026]\n",
            " [0.7112138 ]\n",
            " [0.7112399 ]\n",
            " [0.7113118 ]\n",
            " [0.71126485]\n",
            " [0.7112782 ]\n",
            " [0.7114159 ]\n",
            " [0.7112304 ]\n",
            " [0.7119624 ]\n",
            " [0.7121601 ]\n",
            " [0.71183765]\n",
            " [0.71206945]\n",
            " [0.7127854 ]\n",
            " [0.71290046]\n",
            " [0.7129116 ]\n",
            " [0.7126136 ]\n",
            " [0.7129817 ]\n",
            " [0.71305937]\n",
            " [0.7110128 ]\n",
            " [0.711009  ]\n",
            " [0.710582  ]\n",
            " [0.71062165]\n",
            " [0.71131647]\n",
            " [0.7112395 ]\n",
            " [0.71123403]\n",
            " [0.71164113]\n",
            " [0.71196604]\n",
            " [0.7122122 ]\n",
            " [0.7124146 ]\n",
            " [0.71249366]\n",
            " [0.712443  ]\n",
            " [0.71254325]\n",
            " [0.71257925]\n",
            " [0.71269333]\n",
            " [0.7127775 ]\n",
            " [0.712854  ]\n",
            " [0.71304154]\n",
            " [0.7130619 ]\n",
            " [0.7105942 ]\n",
            " [0.7110802 ]\n",
            " [0.71124446]\n",
            " [0.7112013 ]\n",
            " [0.7112526 ]\n",
            " [0.7107949 ]\n",
            " [0.71127766]\n",
            " [0.71142423]\n",
            " [0.71163785]\n",
            " [0.7121299 ]\n",
            " [0.7121791 ]\n",
            " [0.71234405]\n",
            " [0.71233344]\n",
            " [0.71253777]\n",
            " [0.7126692 ]\n",
            " [0.71280134]\n",
            " [0.7128544 ]\n",
            " [0.71252996]\n",
            " [0.7104396 ]\n",
            " [0.7110076 ]\n",
            " [0.71107256]\n",
            " [0.71101403]\n",
            " [0.7110692 ]\n",
            " [0.7110731 ]\n",
            " [0.71100426]\n",
            " [0.7109496 ]\n",
            " [0.71118057]\n",
            " [0.71124876]\n",
            " [0.71126974]\n",
            " [0.7114258 ]\n",
            " [0.71225184]\n",
            " [0.7122021 ]\n",
            " [0.7126497 ]\n",
            " [0.7124334 ]\n",
            " [0.71268463]\n",
            " [0.7125845 ]\n",
            " [0.7128543 ]\n",
            " [0.71296036]\n",
            " [0.7126607 ]\n",
            " [0.71106964]\n",
            " [0.71120524]\n",
            " [0.71131325]\n",
            " [0.7113327 ]\n",
            " [0.7113019 ]\n",
            " [0.7113685 ]\n",
            " [0.7114523 ]\n",
            " [0.7113997 ]\n",
            " [0.7116468 ]\n",
            " [0.7120261 ]\n",
            " [0.71216273]\n",
            " [0.7123145 ]\n",
            " [0.7128744 ]\n",
            " [0.7111032 ]\n",
            " [0.7108862 ]\n",
            " [0.7110915 ]\n",
            " [0.7109622 ]\n",
            " [0.71087754]\n",
            " [0.7113538 ]\n",
            " [0.71133614]\n",
            " [0.7126149 ]\n",
            " [0.712672  ]\n",
            " [0.7128651 ]\n",
            " [0.71291167]\n",
            " [0.7130368 ]\n",
            " [0.71306884]\n",
            " [0.71096617]\n",
            " [0.7105869 ]\n",
            " [0.71120656]\n",
            " [0.7113855 ]\n",
            " [0.71136236]\n",
            " [0.71143425]\n",
            " [0.71145165]\n",
            " [0.7119615 ]\n",
            " [0.7122848 ]\n",
            " [0.71236074]\n",
            " [0.7125315 ]\n",
            " [0.71255004]\n",
            " [0.71277297]\n",
            " [0.71258414]\n",
            " [0.71306825]\n",
            " [0.7110077 ]\n",
            " [0.7110784 ]\n",
            " [0.71043617]\n",
            " [0.7110767 ]\n",
            " [0.71067023]\n",
            " [0.7111623 ]\n",
            " [0.71135855]\n",
            " [0.7113389 ]\n",
            " [0.71141255]\n",
            " [0.71195245]\n",
            " [0.7121838 ]\n",
            " [0.7123965 ]\n",
            " [0.7124182 ]\n",
            " [0.7126587 ]\n",
            " [0.7127397 ]\n",
            " [0.7127366 ]\n",
            " [0.7127749 ]\n",
            " [0.7125694 ]\n",
            " [0.7128652 ]\n",
            " [0.71295375]\n",
            " [0.7129759 ]\n",
            " [0.7130153 ]\n",
            " [0.71102095]\n",
            " [0.710582  ]\n",
            " [0.7112667 ]\n",
            " [0.7111721 ]\n",
            " [0.7113421 ]\n",
            " [0.71134394]\n",
            " [0.7114698 ]\n",
            " [0.71118814]\n",
            " [0.71161425]\n",
            " [0.7115997 ]\n",
            " [0.7116409 ]\n",
            " [0.7120328 ]\n",
            " [0.71220154]\n",
            " [0.7121867 ]\n",
            " [0.7122359 ]\n",
            " [0.7123275 ]\n",
            " [0.7125815 ]\n",
            " [0.7126466 ]\n",
            " [0.712965  ]\n",
            " [0.71272475]\n",
            " [0.71105015]\n",
            " [0.7110931 ]\n",
            " [0.7111357 ]\n",
            " [0.7111426 ]\n",
            " [0.71124965]\n",
            " [0.7112942 ]\n",
            " [0.7112385 ]\n",
            " [0.7107219 ]\n",
            " [0.71150225]\n",
            " [0.71157193]\n",
            " [0.7120323 ]\n",
            " [0.7121153 ]\n",
            " [0.712212  ]\n",
            " [0.71249926]\n",
            " [0.7125876 ]\n",
            " [0.7125539 ]\n",
            " [0.71287423]\n",
            " [0.7126972 ]\n",
            " [0.71302843]\n",
            " [0.7127047 ]\n",
            " [0.71308607]\n",
            " [0.7113465 ]\n",
            " [0.71123564]\n",
            " [0.7112747 ]\n",
            " [0.7121273 ]\n",
            " [0.71221423]\n",
            " [0.71228546]\n",
            " [0.71230376]\n",
            " [0.71259975]\n",
            " [0.7125982 ]\n",
            " [0.7126281 ]\n",
            " [0.7125016 ]\n",
            " [0.71278656]\n",
            " [0.7130199 ]\n",
            " [0.71304166]\n",
            " [0.7110577 ]\n",
            " [0.7110841 ]\n",
            " [0.71074504]\n",
            " [0.71128815]\n",
            " [0.7115792 ]\n",
            " [0.71187466]\n",
            " [0.71248335]\n",
            " [0.71272135]\n",
            " [0.7128855 ]\n",
            " [0.71303976]\n",
            " [0.711066  ]\n",
            " [0.7112824 ]\n",
            " [0.7107129 ]\n",
            " [0.71095514]\n",
            " [0.7112876 ]\n",
            " [0.71161175]\n",
            " [0.7123221 ]\n",
            " [0.7125095 ]\n",
            " [0.7126569 ]\n",
            " [0.712785  ]\n",
            " [0.71304655]\n",
            " [0.7106272 ]\n",
            " [0.71107715]\n",
            " [0.7110603 ]\n",
            " [0.71107835]\n",
            " [0.7112731 ]\n",
            " [0.7107496 ]\n",
            " [0.7112076 ]\n",
            " [0.71131635]\n",
            " [0.7113196 ]\n",
            " [0.71154666]\n",
            " [0.71120125]\n",
            " [0.71164095]\n",
            " [0.71191853]\n",
            " [0.7125279 ]\n",
            " [0.71253586]\n",
            " [0.7129089 ]\n",
            " [0.7130531 ]\n",
            " [0.7109877 ]\n",
            " [0.7106811 ]\n",
            " [0.7113825 ]\n",
            " [0.712419  ]\n",
            " [0.7125509 ]\n",
            " [0.7129693 ]\n",
            " [0.71298414]\n",
            " [0.713061  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}